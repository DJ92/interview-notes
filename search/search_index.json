{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Engineering Notes","text":"<p>\"The best systems are boring.\"</p> <p>Welcome to my personal knowledge base. This site documents production-grade patterns, trade-offs, and system designs for building scalable software and AI systems.</p> <p>The goal is to move beyond surface-level tutorials and codify the \"Context-as-Code\" required to steer autonomous agents.</p>"},{"location":"#agentic-ai-sdlc","title":"\ud83e\udde0 Agentic AI &amp; SDLC","text":"<p>Standardizing how we build software with AI agents.</p> <ul> <li>AI SDLC: A framework for Context-as-Code (<code>.cursor/rules</code>, Skills, PRDs) and MCP integration to build consistent, high-quality software with agents.</li> </ul>"},{"location":"#machine-learning-systems","title":"\u2699\ufe0f Machine Learning Systems","text":"<p>Productionalizing ML models is harder than training them.</p> <ul> <li>MLflow on Cloud Run: Serverless architecture for ML metadata and model registry.</li> <li>ONNX Java Serving: High-performance CPU &amp; GPU inference in Java environments.</li> </ul>"},{"location":"#developer-experience","title":"\ud83d\udee0\ufe0f Developer Experience","text":"<p>Tools and patterns for efficiency.</p> <ul> <li>Cloud Cheatsheet: CLI references for AWS/GCP/Azure.</li> <li>Zsh Setup: A blazingly fast terminal environment (Powerlevel10k, SDKMAN).</li> </ul>"},{"location":"#about-me","title":"About Me","text":"<p>Maintained by Dheeraj Joshi, a Staff Systems &amp; Machine Learning Engineer focused on large-scale personalization and agentic AI systems.</p>"},{"location":"about/","title":"About","text":"<p>I am a Staff Systems &amp; Machine Learning Engineer specializing in large-scale personalization, ranking, and ML platforms. Also, working on agentic AI systems and research on open source LLMs.</p> <p>Currently, I design and build systems that serve millions of users, focusing on the intersection of theoretical modeling and practical engineering constraints.</p>"},{"location":"about/#technical-focus","title":"Technical Focus","text":""},{"location":"about/#systems-engineering","title":"\ud83c\udfd7 Systems Engineering","text":"<ul> <li>Scalable Architecture: Designing low-latency inference systems.</li> <li>Data Infrastructure: Building robust pipelines for training and real-time serving.</li> <li>MLOps: Automating model lifecycle from research to production.</li> </ul>"},{"location":"about/#applied-machine-learning","title":"\ud83e\udde0 Applied Machine Learning","text":"<ul> <li>Recommender Systems: Deep learning for collaborative filtering and ranking (Two-Tower, DLRM).</li> <li>Natural Language Processing: LLM orchestration and retrieval-augmented generation (RAG).</li> <li>Optimization: Multi-objective optimization for balancing engagement and business metrics.</li> </ul>"},{"location":"about/#agentic-ai-systems","title":"\ud83e\udd16 Agentic AI Systems","text":"<ul> <li>LLM Agents: Building autonomous agents for complex tasks.</li> <li>Open Source LLMs: Research and development of open source LLMs.</li> <li>AI SDLC: Standardizing AI Software Development Lifecycle.</li> </ul>"},{"location":"about/#philosophy","title":"Philosophy","text":"<p>\"Learn, build, share.\"</p> <p>\"The best systems are boring.\"</p> <p>\"Leave the codebase better than you found it.\"</p> <p>\"Hard work beats talent when talent doesn't work hard.\"</p> <p>I believe in simple, predictable, and observable systems. Complexity should be introduced only when necessary to solve a specific problem, not for its own sake.</p>"},{"location":"about/#connect","title":"Connect","text":"<ul> <li>GitHub</li> <li>LinkedIn</li> <li>HuggingFace</li> </ul>"},{"location":"notes/ai-sdlc/","title":"AI SDLC: Context-as-Code","text":"<p>A framework for steering AI agents (Cursor, Claude, Antigravity) to produce consistent, high-quality code by treating context as code.</p>"},{"location":"notes/ai-sdlc/#core-concept","title":"Core Concept","text":"<p>Instead of repeating instructions in every chat session, we commit markdown configuration files to the repository. These files define the rules, skills, and current state for the AI.</p>"},{"location":"notes/ai-sdlc/#1-rules","title":"1. Rules","text":"<p>Rules provide system-level instructions to the Agent. They can be scoped to specific projects or applied globally.</p> <p>Types: - Project Rules (<code>.cursor/rules/*.md</code>): Version-controlled, scoped by path patterns (globs). Best for project-specific conventions. - User Rules (<code>~/.cursor/rules</code> / Dashboard): Global to your environment. Best for personal preferences. - Team Rules (Dashboard): Enforced for all team members.</p> <p>Best Practices: - Keep rules focused and under 500 lines. - Reference files instead of duplicating code. - Avoid vague instructions; be concrete.</p>"},{"location":"notes/ai-sdlc/#2-skills-tool-use","title":"2. Skills (Tool Use)","text":"<p>Skills extend the agent's capabilities by providing it with \"tools\" (functions) it can call to interact with external systems or perform complex logic.</p>"},{"location":"notes/ai-sdlc/#concept","title":"Concept:","text":"<ul> <li>Tool Definitions: JSON schemas defining the function signature (name, description, parameters).</li> <li>Logic: The actual code (e.g., Python script, API call) that executes when the tool is called.</li> </ul>"},{"location":"notes/ai-sdlc/#claudemcp-integration","title":"Claude/MCP Integration:","text":"<ul> <li>Define tools using the Model Context Protocol (MCP) or Claude's native tool format.</li> <li>Agents can \"call\" these tools to fetch documentation, query databases, or run commands.</li> <li>Example: A \"Migrate to Helm\" skill might be a composite of file operations and <code>helm lint</code> commands exposed as a comprehensive tool or workflow.</li> </ul>"},{"location":"notes/ai-sdlc/#3-context-prds-rfcs","title":"3. Context: PRDs &amp; RFCs","text":"<p>Instead of maintaining a scratchpad, provide context through structured documentation.</p>"},{"location":"notes/ai-sdlc/#usage","title":"Usage","text":"<ul> <li>PRDs (Product Requirement Docs): Pass the full PRD to the agent at the start of a feature.</li> <li>RFCs (Request for Comments): Use technical design docs to align the agent on architectural decisions.</li> </ul>"},{"location":"notes/ai-sdlc/#workflow","title":"Workflow","text":"<ol> <li>Prepare: Ensure your PRD or RFC is comprehensive and up-to-date.</li> <li>Inject: <code>@mention</code> the relevant PRD/RFC file in the chat or include it in the context window.</li> <li>Instruct: Ask the agent to \"Implement the feature described in @prd.md\" or \"Review this code against @rfc.md\".</li> </ol>"},{"location":"notes/ai-sdlc/#4-commands-command","title":"4. Commands (<code>/command</code>)","text":"<p>Custom commands allow you to trigger reusable workflows with a simple <code>/</code> prefix in the chat.</p> <p>Locations: - Project: <code>.cursor/commands</code> (Version controlled). - Global: <code>~/.cursor/commands</code> (Personal). - Team: Managed in Cursor Dashboard (Shared).</p> <p>Usage: - <code>/doc</code>: \"Read the code and generate Javadoc/PyDoc.\" - <code>/test</code>: \"Generate unit tests for the selected code.\" - With Parameters: <code>/commit \"fix login bug\"</code> (Passes context to the command).</p>"},{"location":"notes/ai-sdlc/#5-mcp-support","title":"5. MCP Support","text":"<p>Model Context Protocol (MCP) provides a standard interface for agents to connect with external data and tools.</p> Category Supported Integrations Documentation Google Docs, Google Drive (Read specs, access loose notes). Code &amp; Deploy GitHub (PRs, Issues, Actions), Docker (Container management). Project Management JIRA, Linear (Ticket tracking, sprint planning). Communication Slack (Team updates, incident alerts)."},{"location":"notes/ai-sdlc/#mcp-setup-config","title":"MCP Setup Config","text":"<p>Add these to your MCP configuration file (e.g., <code>claude_desktop_config.json</code>).</p> <p>Google Docs &amp; Drive: <pre><code>\"google-drive\": {\n  \"command\": \"npx\",\n  \"args\": [\"-y\", \"@modelcontextprotocol/server-google-drive\"]\n}\n</code></pre></p> <p>GitHub: <pre><code>\"github\": {\n  \"command\": \"npx\",\n  \"args\": [\"-y\", \"@modelcontextprotocol/server-github\"],\n  \"env\": {\n    \"GITHUB_PERSONAL_ACCESS_TOKEN\": \"&lt;YOUR_TOKEN&gt;\"\n  }\n}\n</code></pre></p> <p>Docker: <pre><code>\"docker\": {\n  \"command\": \"docker\",\n  \"args\": [\"run\", \"-i\", \"--rm\", \"mcp/docker\"]\n}\n</code></pre></p> <p>JIRA: <pre><code>\"jira\": {\n  \"command\": \"npx\",\n  \"args\": [\"-y\", \"@modelcontextprotocol/server-jira\"],\n  \"env\": {\n    \"JIRA_API_TOKEN\": \"&lt;YOUR_TOKEN&gt;\",\n    \"JIRA_EMAIL\": \"&lt;YOUR_EMAIL&gt;\",\n    \"JIRA_INSTANCE_URL\": \"&lt;YOUR_INSTANCE_URL&gt;\"\n  }\n}\n</code></pre></p> <p>Linear: <pre><code>\"linear\": {\n  \"command\": \"npx\",\n  \"args\": [\"-y\", \"@modelcontextprotocol/server-linear\"],\n  \"env\": {\n    \"LINEAR_API_KEY\": \"&lt;YOUR_KEY&gt;\"\n  }\n}\n</code></pre></p> <p>Slack: <pre><code>\"slack\": {\n  \"command\": \"npx\",\n  \"args\": [\"-y\", \"@modelcontextprotocol/server-slack\"],\n  \"env\": {\n    \"SLACK_BOT_TOKEN\": \"&lt;YOUR_TOKEN&gt;\",\n    \"SLACK_SIGNING_SECRET\": \"&lt;YOUR_SECRET&gt;\"\n  }\n}\n</code></pre></p>"},{"location":"notes/ai-sdlc/#workflow_1","title":"Workflow","text":"<ol> <li>Start: Agent reads <code>.cursorrules</code> (Style) and consumes context from PRDs/RFCs.</li> <li>Execute: Agent uses Skills and MCP Tools to perform tasks.</li> <li>Commit: Agent validates changes against the PRD/RFC requirements.</li> </ol>"},{"location":"notes/cloud-cheatsheet/","title":"Cloud Service Cheatsheet","text":""},{"location":"notes/cloud-cheatsheet/#gcp-vs-aws-vs-azure-interview-system-design-oriented","title":"GCP vs AWS vs Azure (Interview &amp; System Design Oriented)","text":"<p>This cheatsheet maps core cloud primitives across GCP, AWS, and Azure, with an emphasis on ML systems, data platforms, and production workloads.</p> <p>The goal is not memorizing names, but understanding equivalent building blocks and trade-offs.</p>"},{"location":"notes/cloud-cheatsheet/#compute","title":"Compute","text":"Concept GCP AWS Azure Virtual Machines Compute Engine EC2 Virtual Machines Autoscaling Managed Instance Groups Auto Scaling Groups VM Scale Sets Containers (Managed) GKE EKS / ECS AKS Serverless (HTTP) Cloud Run Lambda Azure Functions Batch Jobs Batch / Dataflow Batch Azure Batch <p>Interview tip: Serverless = fast iteration, bursty workloads VMs / K8s = predictable latency, long-running services</p>"},{"location":"notes/cloud-cheatsheet/#storage","title":"Storage","text":"Concept GCP AWS Azure Object Storage Cloud Storage S3 Blob Storage Block Storage Persistent Disk EBS Managed Disks File Storage Filestore EFS Azure Files <p>ML usage notes - Object storage for datasets, checkpoints, artifacts - Block storage for low-latency model serving - File storage mostly for legacy or shared workloads</p>"},{"location":"notes/cloud-cheatsheet/#databases-data-platforms","title":"Databases &amp; Data Platforms","text":"Concept GCP AWS Azure Managed SQL Cloud SQL RDS Azure SQL Globally Scalable SQL Spanner Aurora Cosmos DB NoSQL (Wide Column / KV) Bigtable DynamoDB Cosmos DB Cache Memorystore ElastiCache Azure Cache for Redis Data Warehouse BigQuery Redshift Synapse <p>Design signal: - OLTP \u2260 Analytics - BigQuery / Redshift / Synapse are not serving databases</p>"},{"location":"notes/cloud-cheatsheet/#messaging-streaming","title":"Messaging &amp; Streaming","text":"Concept GCP AWS Azure Pub/Sub (Eventing) Pub/Sub SNS / SQS Service Bus Streaming Pub/Sub Kinesis Event Hubs Workflow Orchestration Cloud Composer Step Functions Logic Apps <p>ML relevance - Event-driven feature updates - Training triggers - Async inference workflows</p>"},{"location":"notes/cloud-cheatsheet/#networking","title":"Networking","text":"Concept GCP AWS Azure Virtual Network VPC VPC VNet Load Balancer Cloud Load Balancing ALB / NLB Azure Load Balancer DNS Cloud DNS Route 53 Azure DNS CDN Cloud CDN CloudFront Azure CDN <p>Interview note: Talk in terms of L4 vs L7, not product names.</p>"},{"location":"notes/cloud-cheatsheet/#observability-reliability","title":"Observability &amp; Reliability","text":"Concept GCP AWS Azure Logging Cloud Logging CloudWatch Logs Azure Monitor Metrics Cloud Monitoring CloudWatch Metrics Azure Monitor Tracing Cloud Trace X-Ray Application Insights Alerts Alerting Policies CloudWatch Alarms Alerts <p>Senior signal: Observability is a first-class system requirement, not an afterthought.</p>"},{"location":"notes/cloud-cheatsheet/#ml-mlops-platforms-critical-for-mle-interviews","title":"ML &amp; MLOps Platforms (Critical for MLE Interviews)","text":"Capability GCP AWS Azure ML Platform Vertex AI SageMaker Azure ML Training Jobs Vertex Training SageMaker Training Azure ML Jobs Feature Store Vertex Feature Store SageMaker Feature Store Azure Feature Store Pipelines Vertex Pipelines SageMaker Pipelines Azure ML Pipelines Model Registry Vertex Model Registry SageMaker Registry Azure ML Registry Online Serving Vertex Endpoints SageMaker Endpoints Azure Online Endpoints Monitoring Vertex Model Monitoring SageMaker Model Monitor Azure ML Monitoring <p>Key interview insight: Most ML failures come from data + serving, not modeling.</p>"},{"location":"notes/cloud-cheatsheet/#how-i-reason-across-clouds-interview-framing","title":"How I Reason Across Clouds (Interview Framing)","text":"<p>Instead of saying:</p> <p>\u201cI\u2019d use BigQuery\u201d</p> <p>Say:</p> <p>\u201cI\u2019d use a managed columnar data warehouse (BigQuery / Redshift) optimized for analytical queries.\u201d</p> <p>Instead of:</p> <p>\u201cWe use SageMaker\u201d</p> <p>Say:</p> <p>\u201cWe use a managed ML platform for training, registry, deployment, and monitoring.\u201d</p>"},{"location":"notes/cloud-cheatsheet/#cloud-selection-heuristics","title":"Cloud Selection Heuristics","text":"<ul> <li>GCP: Strongest for analytics &amp; ML-native workflows  </li> <li>AWS: Broadest ecosystem, best for infra-heavy systems  </li> <li>Azure: Strong enterprise integration, identity-first</li> </ul> <p>In interviews, emphasize trade-offs, not preferences.</p>"},{"location":"notes/cloud-cheatsheet/#tldr-mental-model","title":"TL;DR Mental Model","text":"<p>All clouds provide the same primitives:</p> <p>Compute \u00b7 Storage \u00b7 Network \u00b7 Data \u00b7 ML \u00b7 Observability</p> <p>If you can reason at that level, service names are just details.</p> <p>This cheatsheet is intended for ML system design interviews and real-world architecture discussions.</p>"},{"location":"notes/llm-evaluation/","title":"Evaluating LLM Outputs: Beyond BLEU Scores","text":"<p>A practical guide to evaluating Large Language Model outputs using automated metrics, LLM-as-judge, and hybrid approaches.</p>"},{"location":"notes/llm-evaluation/#the-challenge","title":"The Challenge","text":"<p>Traditional NLG metrics like BLEU and ROUGE were designed for machine translation and extractive summarization. They fail to capture:</p> <ul> <li>Semantic equivalence: \"Paris is France's capital\" vs \"The capital of France is Paris\"</li> <li>Factual correctness: Plausible but wrong answers</li> <li>Instruction following: Did it actually answer the question?</li> <li>Stylistic quality: Coherence, conciseness, helpfulness</li> </ul>"},{"location":"notes/llm-evaluation/#evaluation-approaches","title":"Evaluation Approaches","text":""},{"location":"notes/llm-evaluation/#1-automated-metrics","title":"1. Automated Metrics","text":""},{"location":"notes/llm-evaluation/#n-gram-overlap-bleu-rouge","title":"N-gram Overlap (BLEU, ROUGE)","text":"<pre><code>from nltk.translate.bleu_score import sentence_bleu\n\nreference = \"The cat sat on the mat\".split()\ncandidate = \"The cat is on the mat\".split()\n\nbleu = sentence_bleu([reference], candidate)  # ~0.75\n</code></pre> <p>Pros: Fast, reproducible, no API costs Cons: Misses semantic similarity, favors lexical overlap</p>"},{"location":"notes/llm-evaluation/#semantic-similarity","title":"Semantic Similarity","text":"<pre><code>from sentence_transformers import SentenceTransformer, util\n\nmodel = SentenceTransformer('all-MiniLM-L6-v2')\n\nref_emb = model.encode(\"Paris is France's capital\")\npred_emb = model.encode(\"The capital of France is Paris\")\n\nsimilarity = util.cos_sim(ref_emb, pred_emb)  # ~0.95\n</code></pre> <p>Pros: Captures semantic equivalence Cons: Not task-specific, no reasoning about correctness</p>"},{"location":"notes/llm-evaluation/#2-llm-as-judge","title":"2. LLM-as-Judge","text":"<p>Use a strong LLM (GPT-4, Claude Opus) to evaluate other LLM outputs.</p> <pre><code>def build_judge_prompt(question: str, response: str) -&gt; str:\n    return f\"\"\"Evaluate this response on CORRECTNESS (1-5):\n\nQuestion: {question}\nResponse: {response}\n\nScore 1-5 where:\n1 = Completely incorrect\n3 = Partially correct\n5 = Completely correct\n\nRespond with JSON: {{\"score\": &lt;1-5&gt;, \"reasoning\": \"&lt;brief explanation&gt;\"}}\"\"\"\n\n# Use Claude to judge\njudge_response = client.messages.create(\n    model=\"claude-opus-4.6\",\n    messages=[{\"role\": \"user\", \"content\": build_judge_prompt(q, r)}]\n)\n</code></pre> <p>Pros: Captures nuanced quality, task-specific evaluation Cons: Expensive, slower, potential bias</p>"},{"location":"notes/llm-evaluation/#3-hybrid-approach","title":"3. Hybrid Approach","text":"<p>Combine both for cost-effective evaluation:</p> <ol> <li>Filter with automated metrics: Remove obviously bad outputs (low BLEU/similarity)</li> <li>Judge high-stakes cases: Use LLM-as-judge for borderline or production-critical outputs</li> <li>Sample for human eval: Validate judge scores on representative subset</li> </ol>"},{"location":"notes/llm-evaluation/#llm-as-judge-best-practices","title":"LLM-as-Judge Best Practices","text":""},{"location":"notes/llm-evaluation/#multi-criteria-evaluation","title":"Multi-Criteria Evaluation","text":"<p>Don't ask for a single \"quality\" score. Separate concerns:</p> <pre><code>CRITERIA = {\n    \"correctness\": \"Is the information factually accurate?\",\n    \"completeness\": \"Does it address all parts of the question?\",\n    \"coherence\": \"Is the logic clear and easy to follow?\",\n    \"conciseness\": \"Is it appropriately brief without redundancy?\",\n    \"helpfulness\": \"Would this actually help the user?\"\n}\n</code></pre>"},{"location":"notes/llm-evaluation/#use-reference-answers-carefully","title":"Use Reference Answers Carefully","text":"<p>With reference: <pre><code>Question: What is 2+2?\nResponse: Four\nReference: The answer is 4\n\nJudge: \"Response is semantically correct despite wording difference. 5/5\"\n</code></pre></p> <p>Without reference (when references are low-quality or unavailable): <pre><code>Question: Explain quantum entanglement simply\nResponse: &lt;candidate answer&gt;\n\nJudge: \"Evaluate based on accuracy, clarity, and appropriate simplicity\"\n</code></pre></p>"},{"location":"notes/llm-evaluation/#structured-output-format","title":"Structured Output Format","text":"<p>Always request JSON for parsing:</p> <pre><code>JUDGE_TEMPLATE = \"\"\"\nEvaluate the response:\n\n{evaluation_criteria}\n\nRespond ONLY with valid JSON:\n{{\n  \"score\": &lt;1-5&gt;,\n  \"reasoning\": \"&lt;1-2 sentence explanation&gt;\",\n  \"key_issues\": [\"issue1\", \"issue2\"] // optional\n}}\n\"\"\"\n</code></pre>"},{"location":"notes/llm-evaluation/#validation-does-llm-as-judge-work","title":"Validation: Does LLM-as-Judge Work?","text":""},{"location":"notes/llm-evaluation/#correlation-with-human-judgment","title":"Correlation with Human Judgment","text":"<p>From my experiments on TruthfulQA (n=200):</p> Judge Model Correlation (\u03c1) Cost/eval GPT-4 0.79 $0.003 Claude Opus 0.82 $0.004 Claude Sonnet 0.76 $0.001 GPT-3.5 0.61 $0.0002 <p>Finding: Sonnet offers best cost-quality tradeoff for most cases.</p>"},{"location":"notes/llm-evaluation/#judge-consistency","title":"Judge Consistency","text":"<p>Test with same inputs across runs:</p> <pre><code>scores = [judge_eval(q, r) for _ in range(10)]\nstd_dev = np.std(scores)  # Should be &lt; 0.3 for temperature=0\n</code></pre> <p>Rule of thumb: Use <code>temperature=0</code> for reproducible evaluation.</p>"},{"location":"notes/llm-evaluation/#bias-detection","title":"Bias Detection","text":"<p>Judges can be biased toward: - Length: Longer = better (even if verbose) - Formatting: Markdown, bullets score higher - Position: First option in pairwise comparisons</p> <p>Mitigation: - Test with intentionally verbose/terse examples - Randomize order in pairwise comparisons - Include explicit anti-verbosity criteria</p>"},{"location":"notes/llm-evaluation/#cost-optimization","title":"Cost Optimization","text":""},{"location":"notes/llm-evaluation/#1-stratified-sampling","title":"1. Stratified Sampling","text":"<p>Don't evaluate everything with LLM-as-judge:</p> <pre><code>def should_judge(auto_scores):\n    \"\"\"Decide if expensive judge is needed.\"\"\"\n    if auto_scores['semantic_sim'] &gt; 0.95:\n        return False  # Clearly good\n    if auto_scores['semantic_sim'] &lt; 0.3:\n        return False  # Clearly bad\n    return True  # Borderline - needs judge\n</code></pre>"},{"location":"notes/llm-evaluation/#2-batch-evaluation","title":"2. Batch Evaluation","text":"<p>Evaluate multiple items in one API call:</p> <pre><code>batch_prompt = f\"\"\"Evaluate these 5 responses in one JSON array:\n\n1. Q: {q1}\\n   R: {r1}\n2. Q: {q2}\\n   R: {r2}\n...\n\nRespond: [{{\"id\": 1, \"score\": X, \"reasoning\": \"...\"}}, ...]\"\"\"\n</code></pre> <p>Savings: ~40% compared to individual calls (due to fixed prompt overhead)</p>"},{"location":"notes/llm-evaluation/#3-cheaper-models-for-simpler-tasks","title":"3. Cheaper Models for Simpler Tasks","text":"Task Complexity Recommended Judge Factual QA Sonnet / GPT-4o-mini Creative writing Opus / GPT-4 Code correctness Opus + execution tests Summarization Sonnet"},{"location":"notes/llm-evaluation/#production-patterns","title":"Production Patterns","text":""},{"location":"notes/llm-evaluation/#pattern-1-fast-automated-sampled-judge","title":"Pattern 1: Fast Automated + Sampled Judge","text":"<pre><code># Evaluate all with automated metrics (fast)\nauto_scores = [automated_eval(r) for r in responses]\n\n# Judge sample for calibration\nsample = random.sample(responses, min(50, len(responses)))\njudge_scores = [llm_judge(r) for r in sample]\n\n# Check correlation\ncorrelation = compute_correlation(auto_scores, judge_scores)\nif correlation &lt; 0.7:\n    warnings.warn(\"Automated metrics may not be reliable\")\n</code></pre>"},{"location":"notes/llm-evaluation/#pattern-2-progressive-evaluation","title":"Pattern 2: Progressive Evaluation","text":"<pre><code>def progressive_eval(response):\n    # Stage 1: Cheap automated filter\n    if automated_score(response) &lt; 0.3:\n        return {\"passed\": False, \"stage\": \"automated\"}\n\n    # Stage 2: Mid-tier judge\n    judge_score = llm_judge(response, model=\"sonnet\")\n    if judge_score &lt; 3.0:\n        return {\"passed\": False, \"stage\": \"sonnet_judge\"}\n\n    # Stage 3: Human review (production-critical only)\n    if is_production_critical:\n        human_score = request_human_eval(response)\n        return {\"passed\": human_score &gt;= 4, \"stage\": \"human\"}\n\n    return {\"passed\": True, \"stage\": \"sonnet_judge\"}\n</code></pre>"},{"location":"notes/llm-evaluation/#pattern-3-ab-testing-with-confidence","title":"Pattern 3: A/B Testing with Confidence","text":"<pre><code>def compare_models(model_a_outputs, model_b_outputs, n_judge=100):\n    \"\"\"Statistical comparison with mixed evaluation.\"\"\"\n\n    # Automated metrics on all\n    auto_a = [automated_eval(r) for r in model_a_outputs]\n    auto_b = [automated_eval(r) for r in model_b_outputs]\n\n    # Judge on sample for high-confidence comparison\n    sample_indices = random.sample(range(len(model_a_outputs)), n_judge)\n    judge_a = [llm_judge(model_a_outputs[i]) for i in sample_indices]\n    judge_b = [llm_judge(model_b_outputs[i]) for i in sample_indices]\n\n    # Statistical test\n    t_stat, p_value = ttest_ind(judge_a, judge_b)\n\n    return {\n        \"auto_mean_a\": np.mean(auto_a),\n        \"auto_mean_b\": np.mean(auto_b),\n        \"judge_mean_a\": np.mean(judge_a),\n        \"judge_mean_b\": np.mean(judge_b),\n        \"significant\": p_value &lt; 0.05\n    }\n</code></pre>"},{"location":"notes/llm-evaluation/#common-pitfalls","title":"Common Pitfalls","text":""},{"location":"notes/llm-evaluation/#using-bleu-for-non-translation-tasks","title":"\u274c Using BLEU for non-translation tasks","text":"<p>Problem: BLEU penalizes valid paraphrases Fix: Use semantic similarity or LLM-as-judge</p>"},{"location":"notes/llm-evaluation/#trusting-judge-without-validation","title":"\u274c Trusting judge without validation","text":"<p>Problem: Judges can be confidently wrong Fix: Validate on human-annotated subset</p>"},{"location":"notes/llm-evaluation/#evaluating-on-training-data","title":"\u274c Evaluating on training data","text":"<p>Problem: Overfitting to specific phrasings Fix: Hold out evaluation set, test distribution shift</p>"},{"location":"notes/llm-evaluation/#ignoring-evaluation-cost","title":"\u274c Ignoring evaluation cost","text":"<p>Problem: Expensive evaluation limits experimentation Fix: Use stratified sampling, cheaper models for filtering</p>"},{"location":"notes/llm-evaluation/#resources","title":"Resources","text":"<ul> <li>Judging LLM-as-a-Judge (Zheng et al., 2023)</li> <li>G-Eval: NLG Evaluation using GPT-4</li> <li>My evaluation framework implementation</li> </ul>"},{"location":"notes/llm-evaluation/#key-takeaways","title":"Key Takeaways","text":"<ol> <li>No single metric is sufficient - use multiple complementary approaches</li> <li>LLM-as-judge is powerful but expensive - use strategically</li> <li>Validate judge scores against human judgment on representative samples</li> <li>Optimize for cost with stratified sampling and model selection</li> <li>Separate evaluation criteria - correctness, completeness, coherence, etc.</li> </ol> <p>This is part of my AI research portfolio exploring practical approaches to production LLM systems.</p>"},{"location":"notes/mlflow-cloud-run/","title":"MLflow in Production on Cloud Run","text":""},{"location":"notes/mlflow-cloud-run/#tracking-registry-and-plugin-based-extensions","title":"Tracking, Registry, and Plugin-Based Extensions","text":"<p>These notes describe a production-grade ML platform setup where MLflow tracking and model registry are deployed as a Cloud Run service, and extended via custom plugins to support enterprise workflows.</p> <pre><code>graph LR\n    subgraph Container [Cloud Run Container]\n        MLflow[MLflow Core]\n        Plugin[Custom Plugins]\n        MLflow &lt;--&gt;|Extends| Plugin\n    end\n\n    User[User / Pipelines] --&gt; Container\n    Container --&gt;|Enables| Enterprise[Enterprise Workflows]\n    style Plugin fill:#f9f,stroke:#333,stroke-width:2px</code></pre> <p>The emphasis is on system architecture, extensibility, and operational tradeoffs rather than tutorial-style deployment.</p>"},{"location":"notes/mlflow-cloud-run/#problem-statement","title":"Problem Statement","text":"<p>Out-of-the-box MLflow works well for: - local experimentation - small teams - notebook-driven workflows</p> <p>It begins to break down when requirements include: - centralized tracking across multiple teams - custom authentication, metadata, and validation - controlled model promotion and governance - cloud-native scalability and reliability</p> <p>This architecture addresses those gaps while keeping MLflow upgradeable.</p>"},{"location":"notes/mlflow-cloud-run/#high-level-architecture","title":"High-Level Architecture","text":"<pre><code>graph TD\n    Client[Clients&lt;br&gt;training jobs, notebooks, CI] --&gt; CR[Cloud Run&lt;br&gt;MLflow Tracking + Registry API]\n\n    subgraph Service[Cloud Run Service]\n        CR\n        Plugin[Plugin layer&lt;br&gt;auth, validation, metadata]\n        CR -.- Plugin\n    end\n\n    CR --&gt; DB[(Backend Store&lt;br&gt;Postgres / Cloud SQL)]\n    CR --&gt; Storage[(Artifact Store&lt;br&gt;GCS)]</code></pre> <p>Key idea: MLflow operates as a stateless control plane, not a monolithic ML system.</p>"},{"location":"notes/mlflow-cloud-run/#why-cloud-run","title":"Why Cloud Run","text":"<p>Cloud Run was chosen because it provides: - Stateless HTTP execution - Automatic horizontal scaling (including scale-to-zero) - Native IAM integration - Simple container-based deployment - Cost efficiency for bursty and spiky traffic</p> <p>This fits MLflow well because: - MLflow APIs are request/response oriented - All state lives in external systems - Training workloads are fully decoupled</p>"},{"location":"notes/mlflow-cloud-run/#core-components","title":"Core Components","text":""},{"location":"notes/mlflow-cloud-run/#1-mlflow-tracking-server","title":"1. MLflow Tracking Server","text":"<ul> <li>Exposes REST APIs for:</li> <li>experiment management</li> <li>run tracking</li> <li>parameter and metric logging</li> <li>Runs as a containerized, stateless service</li> <li>Makes no assumptions about local filesystem persistence</li> </ul>"},{"location":"notes/mlflow-cloud-run/#2-backend-store","title":"2. Backend Store","text":"<p>Responsible for: - experiment metadata - runs - parameters and metrics - model registry state</p> <p>Typical implementations: - Cloud SQL (Postgres) - Managed MySQL</p> <p>Key requirements: - strong transactional guarantees - schema stability across upgrades - automated backups and recovery</p>"},{"location":"notes/mlflow-cloud-run/#3-artifact-store","title":"3. Artifact Store","text":"<p>Responsible for: - model artifacts - checkpoints - feature snapshots - evaluation outputs</p> <p>Typical choice: - Google Cloud Storage (GCS)</p> <p>Design notes: - artifacts are referenced by URI - artifacts are never served directly by the MLflow service - access is controlled via IAM or signed URLs</p>"},{"location":"notes/mlflow-cloud-run/#plugin-based-extension-model","title":"Plugin-Based Extension Model","text":"<p>MLflow plugins are used to extend core behavior without forking MLflow.</p>"},{"location":"notes/mlflow-cloud-run/#why-plugins","title":"Why Plugins","text":"<ul> <li>Avoid long-lived forks of upstream MLflow</li> <li>Preserve a clean upgrade path</li> <li>Isolate organization-specific logic</li> <li>Treat governance as a first-class concern</li> </ul>"},{"location":"notes/mlflow-cloud-run/#common-plugin-responsibilities","title":"Common Plugin Responsibilities","text":""},{"location":"notes/mlflow-cloud-run/#authentication-authorization","title":"Authentication &amp; Authorization","text":"<ul> <li>Validate caller identity</li> <li>Enforce experiment- and registry-level access</li> <li>Integrate with cloud IAM or internal identity systems</li> </ul>"},{"location":"notes/mlflow-cloud-run/#metadata-enrichment","title":"Metadata Enrichment","text":"<ul> <li>Attach required contextual metadata:</li> <li>git SHA</li> <li>training job ID</li> <li>dataset or feature version</li> <li>Enforce mandatory tagging policies</li> </ul>"},{"location":"notes/mlflow-cloud-run/#registry-controls","title":"Registry Controls","text":"<ul> <li>Validate model registration and stage transitions</li> <li>Enforce promotion rules (e.g., staging \u2192 production)</li> <li>Block unsafe or non-compliant transitions</li> </ul>"},{"location":"notes/mlflow-cloud-run/#audit-observability","title":"Audit &amp; Observability","text":"<ul> <li>Emit structured audit logs</li> <li>Track registry actions and transitions</li> <li>Support compliance and forensic analysis</li> </ul>"},{"location":"notes/mlflow-cloud-run/#model-registry-workflow","title":"Model Registry Workflow","text":"<p>Typical promotion flow:</p> <pre><code>graph TD\n    Job[Training Job] --&gt; Log[Log model to MLflow]\n    Log --&gt; Reg[Register versioned model]\n    Reg --&gt; Hook[Plugin validation hooks]\n    Hook --&gt; Stage[Stage transition&lt;br&gt;staging / production]\n    Stage --&gt; Deploy[Downstream deployment automation]</code></pre> <p>Key principle: </p> <p>The model registry is a control surface, not just a metadata store.</p>"},{"location":"notes/mlflow-cloud-run/#integration-with-training-pipelines","title":"Integration with Training Pipelines","text":"<p>MLflow clients are invoked from: - batch training jobs - CI pipelines - scheduled workflows</p> <p>Key design decisions: - clients authenticate using short-lived credentials - training jobs never access databases directly - all interactions flow through the MLflow API</p> <p>This enforces: - consistency - auditability - centralized policy enforcement</p>"},{"location":"notes/mlflow-cloud-run/#operational-considerations","title":"Operational Considerations","text":""},{"location":"notes/mlflow-cloud-run/#scalability","title":"Scalability","text":"<ul> <li>Cloud Run autoscaling absorbs bursty metric logging traffic</li> <li>Database connection pooling is critical</li> <li>Artifact uploads occur out-of-band from request paths</li> </ul>"},{"location":"notes/mlflow-cloud-run/#reliability","title":"Reliability","text":"<ul> <li>Stateless service enables fast restarts and redeployments</li> <li>Backend store is the primary SPOF</li> <li>Health checks and alerts focus on:</li> <li>API latency</li> <li>error rates</li> <li>database connectivity</li> </ul>"},{"location":"notes/mlflow-cloud-run/#security","title":"Security","text":"<ul> <li>No public access to artifacts</li> <li>IAM-scoped service accounts for all components</li> <li>Plugins act as policy enforcement boundaries</li> </ul>"},{"location":"notes/mlflow-cloud-run/#tradeoffs-limitations","title":"Tradeoffs &amp; Limitations","text":""},{"location":"notes/mlflow-cloud-run/#pros","title":"Pros","text":"<ul> <li>Cloud-native and cost-efficient</li> <li>Highly extensible without forking MLflow</li> <li>Clear separation between control plane and data plane</li> <li>Evolves cleanly as governance needs grow</li> </ul>"},{"location":"notes/mlflow-cloud-run/#cons","title":"Cons","text":"<ul> <li>Plugin APIs are lightly documented</li> <li>Debugging plugin behavior requires MLflow internals familiarity</li> <li>Registry workflows still require process discipline and tooling</li> </ul>"},{"location":"notes/mlflow-cloud-run/#interview-takeaways","title":"Interview Takeaways","text":"<p>Key points to emphasize in interviews: - MLflow functions as a metadata and control plane, not a training system - Plugins enable enterprise governance without upstream divergence - Cloud Run is well-suited for stateless ML control services - Most real-world ML complexity lies in data, serving, and governance</p>"},{"location":"notes/mlflow-cloud-run/#tldr","title":"TL;DR","text":"<ul> <li>MLflow runs as a stateless service on Cloud Run</li> <li>Metadata lives in SQL; artifacts live in object storage</li> <li>Plugins enforce auth, validation, and policy</li> <li>The registry acts as a production gate</li> <li>Designed for scale, auditability, and fast iteration</li> </ul> <p>These notes reflect a real production ML platform, not a tutorial deployment.</p>"},{"location":"notes/onnx-java-serving/","title":"ONNX Conversion &amp; Java Serving (CPU &amp; GPU)","text":"<p>High-performance inference on CPU and GPU using ONNX Runtime (ORT) in a Java environment.</p>"},{"location":"notes/onnx-java-serving/#why-onnx-java","title":"Why ONNX + Java?","text":"<ul> <li>JVM Ecosystem: Integrate directly with existing Java/Scala backend services (Spring Boot, Flink, Spark).</li> <li>Lower Latency: Avoid HTTP overhead of calling an external model server (TensorFlow Serving / TorchServe) by running in-process (JNI).</li> <li>CPU optimization: ONNX Runtime is highly optimized for CPU inference (AVX2/AVX512).</li> </ul>"},{"location":"notes/onnx-java-serving/#1-model-conversion-python","title":"1. Model Conversion (Python)","text":"<p>Convert PyTorch or TensorFlow models to <code>.onnx</code> format.</p>"},{"location":"notes/onnx-java-serving/#pytorch-to-onnx","title":"PyTorch to ONNX","text":"<pre><code>import torch\nimport torch.nn as nn\n\n# 1. Load trained model\nmodel = MyModel()\nmodel.load_state_dict(torch.load(\"model.pth\"))\nmodel.eval()\n\n# 2. Define dummy input (shape must match model input)\ndummy_input = torch.randn(1, 3, 224, 224)\n\n# 3. Export\ntorch.onnx.export(\n    model, \n    dummy_input, \n    \"model.onnx\",\n    input_names=[\"input\"], \n    output_names=[\"output\"],\n    dynamic_axes={\"input\": {0: \"batch_size\"}, \"output\": {0: \"batch_size\"}},\n    opset_version=14\n)\n</code></pre>"},{"location":"notes/onnx-java-serving/#tensorflow-to-onnx","title":"TensorFlow to ONNX","text":"<p>Use <code>tf2onnx</code>:</p> <pre><code>pip install tf2onnx\npython -m tf2onnx.convert --saved-model ./tf_model --output model.onnx --opset 14\n</code></pre>"},{"location":"notes/onnx-java-serving/#2-java-inference-onnx-runtime","title":"2. Java Inference (ONNX Runtime)","text":"<p>Use the <code>onnxruntime</code> Java API to load and query the model.</p>"},{"location":"notes/onnx-java-serving/#dependency-maven","title":"Dependency (Maven)","text":"<pre><code>&lt;!-- Optimized Native Binaries for CPU --&gt;\n&lt;dependency&gt;\n    &lt;groupId&gt;com.microsoft.onnxruntime&lt;/groupId&gt;\n    &lt;artifactId&gt;onnxruntime&lt;/artifactId&gt;\n    &lt;version&gt;1.17.1&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre>"},{"location":"notes/onnx-java-serving/#inference-code","title":"Inference Code","text":"<pre><code>import ai.onnxruntime.*;\nimport java.nio.FloatBuffer;\nimport java.util.Collections;\n\npublic class ModelService {\n    private OrtEnvironment env;\n    private OrtSession session;\n\n    public ModelService(String modelPath) throws OrtException {\n        this.env = OrtEnvironment.getEnvironment();\n        // Optimization: Enable graph optimizations\n        OrtSession.SessionOptions opts = new OrtSession.SessionOptions();\n        opts.setOptimizationLevel(OrtSession.SessionOptions.OptLevel.ALL_OPT);\n        opts.setIntraOpNumThreads(4); // Tune based on CPU cores\n\n        this.session = env.createSession(modelPath, opts);\n    }\n\n    public float[] predict(float[] inputData, long[] shape) throws OrtException {\n        // 1. Create Tensor from Java array\n        OnnxTensor inputTensor = OnnxTensor.createTensor(env, FloatBuffer.wrap(inputData), shape);\n\n        // 2. Run Inference\n        OrtSession.Result result = session.run(Collections.singletonMap(\"input\", inputTensor));\n\n        // 3. Extract Output\n        float[][] output = (float[][]) result.get(0).getValue();\n        result.close(); // Important: Close native resource to prevent leaks (or use try-with-resources)\n\n        return output[0];\n    }\n}\n</code></pre>"},{"location":"notes/onnx-java-serving/#3-java-inference-gpu-cuda","title":"3. Java Inference (GPU / CUDA)","text":"<p>To run on NVIDIA GPUs, switch dependencies and configure the session options.</p>"},{"location":"notes/onnx-java-serving/#dependency-maven_1","title":"Dependency (Maven)","text":"<p>Replace <code>onnxruntime</code> with <code>onnxruntime_gpu</code>:</p> <pre><code>&lt;dependency&gt;\n    &lt;groupId&gt;com.microsoft.onnxruntime&lt;/groupId&gt;\n    &lt;artifactId&gt;onnxruntime_gpu&lt;/artifactId&gt;\n    &lt;version&gt;1.17.1&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre>"},{"location":"notes/onnx-java-serving/#cuda-configuration-code","title":"CUDA Configuration Code","text":"<pre><code>import ai.onnxruntime.providers.OrtCUDAProviderOptions;\n\n// ... inside constructor ...\n\nOrtSession.SessionOptions opts = new OrtSession.SessionOptions();\n\n// 1. Configure CUDA Provider\nOrtCUDAProviderOptions cudaOpts = new OrtCUDAProviderOptions(0); // GPU Device ID 0\nopts.addCUDA(cudaOpts);\n\n// 2. Create Session (will load model onto GPU)\nthis.session = env.createSession(modelPath, opts);\n</code></pre> <p>Note: Ensure the host machine has compatible NVIDIA Drivers and CUDA Toolkit installed (matching the ORT version).</p>"},{"location":"notes/onnx-java-serving/#4-performance-tuning","title":"4. Performance Tuning","text":""},{"location":"notes/onnx-java-serving/#cpu-tuning","title":"CPU Tuning","text":"Setting Recommendation <code>IntraOpNumThreads</code> Set to number of physical cores available to the request. Don't oversubscribe. <code>InterOpNumThreads</code> Keep low (1) unless running parallel subgraphs (rare). Execution Mode <code>SEQUENTIAL</code> is usually faster for simple models; <code>PARALLEL</code> increases latency but throughput for complex graphs. Memory Mapping Use <code>sessionOptions.addSessionConfigEntry(\"session.load_model_format\", \"ORT\")</code> for faster loading if using optimized format."},{"location":"notes/onnx-java-serving/#gpu-tuning","title":"GPU Tuning","text":"Setting Recommendation <code>cudnn_conv_algo_search</code> Set to <code>HEURISTIC</code> (default) or <code>EXHAUSTIVE</code> (slower init, faster run) via provider options. <code>gpu_mem_limit</code> Set a hard limit on GPU memory usage if sharing the card with other processes. IO Binding Crucial: Use <code>run(..., runOptions, ioBinding)</code> to keep inputs/outputs on device (GPU) and avoid CPU-GPU copies."},{"location":"notes/onnx-java-serving/#memory-management","title":"Memory Management","text":"<p>Critical: ONNX Runtime uses off-heap memory (C++).  - Always close <code>OrtSession.Result</code> and <code>OnnxTensor</code> objects explicitly or use <code>try-with-resources</code>.  - Garbage Collection (GC) won't reclaim native memory fast enough, leading to OOM.</p>"},{"location":"notes/onnx-java-serving/#5-advanced-topics","title":"5. Advanced Topics","text":""},{"location":"notes/onnx-java-serving/#quantization-int8","title":"Quantization (INT8)","text":"<p>Drastically reduce model size and latency with minimal accuracy loss. - Dynamic Quantization: Weights are INT8, activations quantized on-the-fly (Great for NLP/Transformers). - Static Quantization: Weights and activations are INT8. Requires a \"calibration\" dataset to determine ranges (Best for CNNs).</p>"},{"location":"notes/onnx-java-serving/#execution-providers-cuda-vs-tensorrt","title":"Execution Providers: CUDA vs TensorRT","text":"<ul> <li>CUDA: General purpose, robust support for most ONNX ops. Fast compilation.</li> <li>TensorRT: NVIDIA's specialized optimizer.<ul> <li>Pros: Can be ~2-5x faster than basic CUDA execution.</li> <li>Cons: Extremely slow engine build time (minutes on startup), strictly tied to specific GPU architecture. Use for stable, long-running production deployments.</li> </ul> </li> </ul>"},{"location":"notes/onnx-java-serving/#6-architecture-diagram","title":"6. Architecture Diagram","text":"<pre><code>graph TD\n    subgraph Python [\"Training Phase - Python\"]\n        TF[\"TensorFlow / PyTorch\"] --&gt;|Export| ModelFile[\"model.onnx\"]\n    end\n\n    subgraph Java [\"Serving Phase - JVM\"]\n        AppService[\"Spring Boot / Flink\"] \n        OrtRuntime[\"ONNX Runtime C++\"]\n\n        ModelFile --&gt;|Load| OrtRuntime\n        AppService -- JNI --&gt; OrtRuntime\n        OrtRuntime -- Result --&gt; AppService\n    end\n\n    style OrtRuntime fill:#f9f,stroke:#333</code></pre>"},{"location":"notes/zsh-setup/","title":"MacOS Dev Environment Setup","text":"<p>My local Zsh configuration using Oh My Zsh, Powerlevel10k, and various version managers.</p>"},{"location":"notes/zsh-setup/#1-shell-theme","title":"1. Shell &amp; Theme","text":"<ul> <li>Shell: Zsh (Default on macOS)</li> <li>Framework: Oh My Zsh</li> <li>Theme: Powerlevel10k<ul> <li>Configuration: <code>.p10k.zsh</code> (Run <code>p10k configure</code> to customize)</li> </ul> </li> </ul>"},{"location":"notes/zsh-setup/#2-plugins","title":"2. Plugins","text":"<p>Plugins configured in <code>.zshrc</code>:</p> Plugin Purpose <code>git</code> Aliases and functions for git (e.g., <code>gst</code> for <code>git status</code>). <code>you-should-use</code> Reminds you of existing aliases for commands you type. <code>zsh-autosuggestions</code> Suggests commands as you type based on history. <code>zsh-syntax-highlighting</code> Highlights commands in green (valid) or red (invalid). <code>zsh-bat</code> Integration with <code>bat</code> (a better <code>cat</code> with syntax highlighting). <code>nvm</code> Zsh plugin for NVM lazy loading."},{"location":"notes/zsh-setup/#3-version-managers","title":"3. Version Managers","text":""},{"location":"notes/zsh-setup/#nodejs-nvm","title":"Node.js (NVM)","text":"<ul> <li>Tool: <code>nvm</code> (Node Version Manager)</li> <li>Path: <code>~/.nvm</code></li> <li>Usage:     <pre><code>nvm install --lts   # Install latest LTS\nnvm use &lt;version&gt;   # Switch version\n</code></pre></li> </ul>"},{"location":"notes/zsh-setup/#java-kotlin-sdkman","title":"Java / Kotlin (SDKMAN)","text":"<ul> <li>Tool: SDKMAN!</li> <li>Path: <code>~/.sdkman</code></li> <li>Usage:     <pre><code>sdk list java       # List available JDKs\nsdk install java 21.0.2-tem # Install Temurin JDK 21\n</code></pre></li> </ul>"},{"location":"notes/zsh-setup/#python-pyenv","title":"Python (Pyenv)","text":"<ul> <li>Tool: <code>pyenv</code></li> <li>Path: <code>~/.pyenv</code></li> <li>Usage:     <pre><code>pyenv install 3.11.0\npyenv global 3.11.0\n</code></pre></li> </ul>"},{"location":"notes/zsh-setup/#4-custom-paths-exports","title":"4. Custom Paths &amp; Exports","text":"<ul> <li>Antigravity: Added to path.</li> <li>Hugging Face CLI: Added to path.</li> </ul> <pre><code># Example alias\nalias zshconfig=\"nano ~/.zshrc\"\nalias ohmyzsh=\"nano ~/.oh-my-zsh\"\n</code></pre>"}]}